{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = 3 \n",
      "b = 5 \n",
      "a + b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "8"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import findspark\n",
      "findspark.init('/usr/lib/spark/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pyspark\n",
      "sc = pyspark.SparkContext()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 22:38:20,583 WARN  [Thread-2] util.Utils (Logging.scala:logWarning(71)) - Your hostname, quickstart.cloudera resolves to a loopback address: 127.0.0.1; using 192.168.188.128 instead (on interface eth1)\n",
        "2016-08-26 22:38:20,597 WARN  [Thread-2] util.Utils (Logging.scala:logWarning(71)) - Set SPARK_LOCAL_IP if you need to bind to another address\n",
        "2016-08-26 22:38:20,936 INFO  [Thread-2] spark.SecurityManager (Logging.scala:logInfo(59)) - Changing view acls to: cloudera\n",
        "2016-08-26 22:38:20,937 INFO  [Thread-2] spark.SecurityManager (Logging.scala:logInfo(59)) - Changing modify acls to: cloudera\n",
        "2016-08-26 22:38:20,937 INFO  [Thread-2] spark.SecurityManager (Logging.scala:logInfo(59)) - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)\n",
        "2016-08-26 22:38:21,441 INFO  [sparkDriver-akka.actor.default-dispatcher-3] slf4j.Slf4jLogger (Slf4jLogger.scala:applyOrElse(80)) - Slf4jLogger started\n",
        "2016-08-26 22:38:21,541 INFO  [sparkDriver-akka.actor.default-dispatcher-5] Remoting (Slf4jLogger.scala:apply$mcV$sp(74)) - Starting remoting\n",
        "2016-08-26 22:38:21,735 INFO  [sparkDriver-akka.actor.default-dispatcher-4] Remoting (Slf4jLogger.scala:apply$mcV$sp(74)) - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.188.128:58669]\n",
        "2016-08-26 22:38:21,736 INFO  [sparkDriver-akka.actor.default-dispatcher-4] Remoting (Slf4jLogger.scala:apply$mcV$sp(74)) - Remoting now listens on addresses: [akka.tcp://sparkDriver@192.168.188.128:58669]\n",
        "2016-08-26 22:38:21,751 INFO  [Thread-2] util.Utils (Logging.scala:logInfo(59)) - Successfully started service 'sparkDriver' on port 58669.\n",
        "2016-08-26 22:38:21,771 INFO  [Thread-2] spark.SparkEnv (Logging.scala:logInfo(59)) - Registering MapOutputTracker\n",
        "2016-08-26 22:38:21,781 INFO  [Thread-2] spark.SparkEnv (Logging.scala:logInfo(59)) - Registering BlockManagerMaster\n",
        "2016-08-26 22:38:21,808 INFO  [Thread-2] storage.DiskBlockManager (Logging.scala:logInfo(59)) - Created local directory at /tmp/spark-local-20160826223821-6c2e\n",
        "2016-08-26 22:38:21,867 INFO  [Thread-2] util.Utils (Logging.scala:logInfo(59)) - Successfully started service 'Connection manager for block manager' on port 56635.\n",
        "2016-08-26 22:38:21,869 INFO  [Thread-2] network.ConnectionManager (Logging.scala:logInfo(59)) - Bound socket to port 56635 with id = ConnectionManagerId(192.168.188.128,56635)\n",
        "2016-08-26 22:38:21,874 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - MemoryStore started with capacity 267.3 MB\n",
        "2016-08-26 22:38:21,891 INFO  [Thread-2] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Trying to register BlockManager\n",
        "2016-08-26 22:38:21,893 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMasterActor (Logging.scala:logInfo(59)) - Registering block manager 192.168.188.128:56635 with 267.3 MB RAM\n",
        "2016-08-26 22:38:21,898 INFO  [Thread-2] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Registered BlockManager\n",
        "2016-08-26 22:38:21,920 INFO  [Thread-2] spark.HttpFileServer (Logging.scala:logInfo(59)) - HTTP File server directory is /tmp/spark-c5dc06a4-8401-47a1-b0d1-040fcabce50a\n",
        "2016-08-26 22:38:21,924 INFO  [Thread-2] spark.HttpServer (Logging.scala:logInfo(59)) - Starting HTTP Server\n",
        "2016-08-26 22:38:22,071 INFO  [Thread-2] server.Server (Server.java:doStart(272)) - jetty-8.y.z-SNAPSHOT\n",
        "2016-08-26 22:38:22,107 INFO  [Thread-2] server.AbstractConnector (AbstractConnector.java:doStart(338)) - Started SocketConnector@0.0.0.0:54512\n",
        "2016-08-26 22:38:22,107 INFO  [Thread-2] util.Utils (Logging.scala:logInfo(59)) - Successfully started service 'HTTP file server' on port 54512.\n",
        "2016-08-26 22:38:22,473 INFO  [Thread-2] server.Server (Server.java:doStart(272)) - jetty-8.y.z-SNAPSHOT\n",
        "2016-08-26 22:38:22,483 INFO  [Thread-2] server.AbstractConnector (AbstractConnector.java:doStart(338)) - Started SelectChannelConnector@0.0.0.0:4040\n",
        "2016-08-26 22:38:22,483 INFO  [Thread-2] util.Utils (Logging.scala:logInfo(59)) - Successfully started service 'SparkUI' on port 4040.\n",
        "2016-08-26 22:38:22,486 INFO  [Thread-2] ui.SparkUI (Logging.scala:logInfo(59)) - Started SparkUI at http://192.168.188.128:4040\n",
        "2016-08-26 22:38:23,147 WARN  [Thread-2] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
        "2016-08-26 22:38:23,721 INFO  [sparkDriver-akka.actor.default-dispatcher-5] util.AkkaUtils (Logging.scala:logInfo(59)) - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.188.128:58669/user/HeartbeatReceiver\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## First Spark Program"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd=sc.parallelize([1,2,3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.reduce(lambda a,b: a* b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 22:40:06,609 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: reduce at <ipython-input-5-b44a8bdc7f8c>:1\n",
        "2016-08-26 22:40:06,636 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 0 (reduce at <ipython-input-5-b44a8bdc7f8c>:1) with 2 output partitions (allowLocal=false)\n",
        "2016-08-26 22:40:06,638 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 0(reduce at <ipython-input-5-b44a8bdc7f8c>:1)\n",
        "2016-08-26 22:40:06,638 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List()\n",
        "2016-08-26 22:40:06,651 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List()\n",
        "2016-08-26 22:40:06,663 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 0 (PythonRDD[1] at RDD at PythonRDD.scala:43), which has no missing parents\n",
        "2016-08-26 22:40:06,728 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3528) called with curMem=0, maxMem=280248975\n",
        "2016-08-26 22:40:06,730 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 267.3 MB)\n",
        "2016-08-26 22:40:06,899 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 0 (PythonRDD[1] at RDD at PythonRDD.scala:43)\n",
        "2016-08-26 22:40:06,900 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 0.0 with 2 tasks\n",
        "2016-08-26 22:40:06,928 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1060 bytes)\n",
        "2016-08-26 22:40:06,942 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1067 bytes)\n",
        "2016-08-26 22:40:06,945 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 0.0 (TID 0)\n",
        "2016-08-26 22:40:06,947 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 0.0 (TID 1)\n",
        "2016-08-26 22:40:07,260 INFO  [Executor task launch worker-1] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 259, boot = 242, init = 16, finish = 1\n",
        "2016-08-26 22:40:07,276 INFO  [Executor task launch worker-0] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 274, boot = 262, init = 12, finish = 0\n",
        "2016-08-26 22:40:07,296 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 0.0 (TID 0). 621 bytes result sent to driver\n",
        "2016-08-26 22:40:07,297 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 0.0 (TID 1). 621 bytes result sent to driver\n",
        "2016-08-26 22:40:07,318 INFO  [Result resolver thread-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 0.0 (TID 0) in 390 ms on localhost (1/2)\n",
        "2016-08-26 22:40:07,327 INFO  [Result resolver thread-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 0.0 (TID 1) in 387 ms on localhost (2/2)\n",
        "2016-08-26 22:40:07,328 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 0 (reduce at <ipython-input-5-b44a8bdc7f8c>:1) finished in 0.418 s\n",
        "2016-08-26 22:40:07,334 INFO  [Result resolver thread-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
        "2016-08-26 22:40:07,341 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Job finished: reduce at <ipython-input-5-b44a8bdc7f8c>:1, took 0.730569943 s\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "6"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Word Count Example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "input = sc.textFile(\"file:///home/cloudera/cnn.txt\")\n",
      "words = input.map(lambda x: x.upper())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 22:58:10,307 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(177621) called with curMem=3528, maxMem=280248975\n",
        "2016-08-26 22:58:10,312 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_1 stored as values in memory (estimated size 173.5 KB, free 267.1 MB)\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "PythonRDD[4] at RDD at PythonRDD.scala:43"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print words.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u\"(CNN)THE INTERNATIONAL PARALYMPIC COMMITTEE (IPC) HAS BROUGHT HUMILIATION ON ITSELF BY OPTING TO BAN RUSSIA FROM PARTICIPATING IN THE 2016 PARALYMPIC GAMES, ACCORDING TO THE COUNTRY'S PRESIDENT VLADIMIR PUTIN.\", u'', u'THE IPC IMPOSED THE SUSPENSION THIS MONTH DUE TO ALLEGED WIDESPREAD STATE-SPONSORED VIOLATIONS OF INTERNATIONAL DOPING RULES.', u\"RUSSIA IMMEDIATELY APPEALED AGAINST THE BAN ON ITS PARALYMPIC ATHLETES COMPETING IN RIO DE JANEIRO THIS MONTH, BUT THE COURT OF ARBITRATION FOR SPORT (CAS) UPHELD THE IPC'S DECISION.\", u'RUSSIA PARALYMPIC BAN UPHELD', u'', u'RUSSIA PARALYMPIC BAN UPHELD 01:48', u'\"I FEEL SORRY FOR THOSE WHO MAKE SUCH DECISIONS AS THEY CANNOT BUT UNDERSTAND THAT THEY ARE HUMILIATING THEMSELVES,\" PUTIN SAID AT AN EVENT IN MOSCOW THURSDAY, ACCORDING TO RUSSIA\\'S STATE-RUN TASS NEWS AGENCY.', u'\"THE DECISION TO SUSPEND OUR PARALYMPIANS IS BEYOND ALL BOUNDARIES OF LEGAL NORMS, MORAL PRINCIPLES AND HUMANITY.', u'\"THIS IS SIMPLY CYNICAL TO WORK FRUSTRATION OFF ON THOSE WHO SEE SPORT AS THEIR SENSE TO LIVE, THOSE WHO INSPIRE WITH HOPE AND BELIEF MILLIONS OF PEOPLE WITH LIMITED PHYSICAL ABILITIES.\"', u'PROTEST PETITION', u'WHEN FIRST ANNOUNCING THE BAN, IPC PRESIDENT PHILIP CRAVEN SAID THE GOVERNING BODY HAD \"ENORMOUS SYMPATHY\" FOR THE ATHLETES MISSING OUT ON THE PARALYMPICS, BUT THEY HAD ULTIMATELY BEEN FAILED BY RUSSIA\\'S \"MEDALS-OVER-MORALS MENTALITY.\"', u'MANY IN RUSSIA DISAGREE WITH CRAVEN AND THE IPC, HOWEVER, WITH A ONLINE PETITION LABELING THE BAN \"INHUMAN\" AND CALLING FOR RUSSIAN ATHLETES TO BE ALLOWED TO COMPETE IN RIO INDEPENDENTLY. IT HAS RECEIVED 300,000 SIGNATURES OF SUPPORT.', u'RUSSIA BANNED FROM PARALYMPIC GAMES AFTER APPEAL DENIED', u'', u'RUSSIA BANNED FROM PARALYMPIC GAMES AFTER APPEAL DENIED 01:22', u'RUSSIAN ATHLETES BARRED FROM PARALYMPIC GAMES', u'', u'RUSSIAN ATHLETES BARRED FROM PARALYMPIC GAMES 00:46', u'SEVERAL RUSSIAN DISABLED CHARITIES, INCLUDING THE ALL-RUSSIAN SOCIETY OF DISABLED PEOPLE AND THE ALL-RUSSIAN SOCIETY OF THE DEAF, HAVE ALSO URGED THE IPC IN AN OPEN LETTER TO RECONSIDER ITS STANCE.', u'KSENIYA ALFEROVA, WHO HELPED START THE PETITION, SAYS SHE FEELS RUSSIAN PARALYMPIANS HAVE BEEN LET DOWN BY THE IPC AND CAS.', u'\"I\\'M FRIENDS WITH A LOT OF RUSSIAN PARALYMPIANS AND THEIR FIRST REACTION (TO THE CAS DECISION) IS THAT THEY DON\\'T EVEN WANT TO TALK, BECAUSE THEY ARE SO SHOCKED,\" ALFEROVA TOLD CNN.', u'\"THE ATHLETES ARE VERY STRONG, AND I THINK SOME OF THEM WILL RECOVER. BUT I\\'M AFRAID SOME OF THEM WILL NOT.', u'\"IT\\'S A VERY DIFFICULT STEP FOR THEM, BECAUSE THEY DIDN\\'T BELIEVE IN ANYTHING. NOW THEY DO. NOW THEY HAVE FAMILIES, THEY BELIEVE IN THE FUTURE. AND THIS IS TAKING THE FUTURE FROM THEM.\"', u\"WHILE 118 OF THE 389-STRONG RUSSIAN TEAM WERE BANNED FROM COMPETING AT THIS MONTH'S RIO OLYMPICS DUE TO THE DOPING ALLEGATIONS, WITH INDIVIDUAL FEDERATIONS GIVEN THE RESPONSIBILITY FOR CLEARING ATHLETES, THE IPC IMPLEMENTED A BLANKET BAN.\", u'ALFEROVA FEELS THE PARALYMPIANS SHOULD HAVE BEEN HANDLED DIFFERENTLY.', u'\"EVERYONE WAS SURE THE DECISION WOULD BE DIFFERENT,\" SHE SAID. \"IT HAS NOTHING TO DO WITH THE COUNTRY; IT HAS TO DO WITH THE PEOPLE. THE PARALYMPIC GAMES ARE NOT COUNTRIES, OR POLITICS -- IT\\'S PEOPLE. IT\\'S NOT JUSTICE. IT\\'S NOT RIGHT.\"', u'NO U-TURN', u\"THE IPC HAS SINCE RESPONDED TO ALFEROVA'S PETITION, WITH SPOKESMAN CRAIG SPENCE STATING THAT RUSSIAN PARALYMPIANS WILL NOT BE ALLOWED TO COMPETE INDEPENDENTLY FROM THE RUSSIAN PARALYMPIC COMMITTEE.\", u'\"UNDER THE IPC\\'S RULES, ONE MEMBER IS RESPONSIBLE FOR RUSSIAN ATHLETES AND THAT IS THE RUSSIAN PARALYMPIC COMMITTEE,\" SPENCE SAID.', u'THE RUSSIANS GOING TO RIO ... AND SOME WHO AREN&#39;T', u\"PHOTOS: THE RUSSIANS GOING TO RIO ... AND SOME WHO AREN'T\", u'\"IT HAS BEEN SUSPENDED DUE TO ITS INABILITY TO FULFILL ITS IPC MEMBERSHIP RESPONSIBILITIES AND OBLIGATIONS. IF THAT MEMBER IS SUSPENDED, THEN THEY CANNOT ENTER ATHLETES INTO THE GAMES.\"', u\"SPENCE ALSO ECHOED CRAVEN'S COMMENTS WHEN ADDING THAT THE ATHLETES HAVE BEEN CHEATED OUT OF A PLACE AT RIO BY RUSSIA'S ALLEGED DOPING VIOLATIONS.\", u'\"THE ATHLETES SHOULD BE UPSET AND ANGRY WITH THE OFFICIALS BEHIND THIS STATE-SPONSORED DOPING SYSTEM IN RUSSIA,\" HE TOLD CNN.', u'\"IT IS BECAUSE OF THIS SYSTEM THAT THE RUSSIAN PARALYMPIC COMMITTEE IS SUSPENDED. THE SYSTEM NEEDS TO CHANGE IF THE SUSPENSION IS TO BE LIFTED.\"', u'BROKEN DREAMS', u'LUDMILA BUBNOVA, WHO WAS DUE TO BE PART OF RUSSIA\\'S FIRST WOMEN\\'S WHEELCHAIR TENNIS PARALYMPICS TEAM, BELIEVES AN \"UNFAIR DECISION\" FROM THE IPC HAS COST HER THAT DREAM.', u'IAAF PRESIDENT EXPLAINS DOPING BANS', u'', u'IAAF PRESIDENT EXPLAINS DOPING BANS 02:37', u'\"I COULDN\\'T BELIEVE IT AT FIRST. I THOUGHT IT WAS SOME SORT OF MISTAKE,\" SHE TOLD CNN. \"I UNDERWENT DOPING TESTS SO MANY TIMES AND WAS NEVER CAUGHT UP ON ANYTHING. SO WHY SHOULD I BE BANNED FROM PARALYMPICS IF I DON\\'T DOPE?', u'\"I THINK THE DECISION WAS MADE WITH POLITICS INVOLVED. IT\\'S AN UNFAIR DECISION. I BELIEVE THAT EVERY ATHLETE SHOULD BE RESPONSIBLE FOR HIMSELF ONLY. WHY ARE YOU PUNISHING INNOCENT (PEOPLE)?\"', u'', u'HAVE YOUR SAY ON OUR FACEBOOK PAGE', u'\"MY DREAM WAS TO COMPETE AT THE PARALYMPICS. SPORTS MEANS EVERYTHING TO ME, I COULDN\\'T LIVE WITHOUT SPORT,\" BUBNOVA ADDED. \"WE\\'LL KEEP ON TRAINING, WE\\'LL KEEP ON FIGHTING, WE\\'LL KEEP ON PLAYING -- IT\\'S LIFE. SPORT IS OUR LIFE.\"']\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 22:58:36,595 INFO  [Thread-2] mapred.FileInputFormat (FileInputFormat.java:listStatus(247)) - Total input paths to process : 1\n",
        "2016-08-26 22:58:36,612 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: collect at <ipython-input-9-f03a6e17fb33>:1\n",
        "2016-08-26 22:58:36,613 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 1 (collect at <ipython-input-9-f03a6e17fb33>:1) with 2 output partitions (allowLocal=false)\n",
        "2016-08-26 22:58:36,613 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 1(collect at <ipython-input-9-f03a6e17fb33>:1)\n",
        "2016-08-26 22:58:36,613 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List()\n",
        "2016-08-26 22:58:36,614 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List()\n",
        "2016-08-26 22:58:36,615 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 1 (PythonRDD[4] at RDD at PythonRDD.scala:43), which has no missing parents\n",
        "2016-08-26 22:58:36,625 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(4336) called with curMem=181149, maxMem=280248975\n",
        "2016-08-26 22:58:36,626 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_2 stored as values in memory (estimated size 4.2 KB, free 267.1 MB)\n",
        "2016-08-26 22:58:36,635 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 1 (PythonRDD[4] at RDD at PythonRDD.scala:43)\n",
        "2016-08-26 22:58:36,635 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 1.0 with 2 tasks\n",
        "2016-08-26 22:58:36,637 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 1.0 (TID 2, localhost, PROCESS_LOCAL, 1183 bytes)\n",
        "2016-08-26 22:58:36,638 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 1.0 (TID 3, localhost, PROCESS_LOCAL, 1183 bytes)\n",
        "2016-08-26 22:58:36,638 INFO  [Executor task launch worker-2] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 1.0 (TID 2)\n",
        "2016-08-26 22:58:36,642 INFO  [Executor task launch worker-3] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 1.0 (TID 3)\n",
        "2016-08-26 22:58:36,681 INFO  [stdout writer for /usr/bin/python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/home/cloudera/cnn.txt:2570+2571\n",
        "2016-08-26 22:58:36,681 INFO  [stdout writer for /usr/bin/python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/home/cloudera/cnn.txt:0+2570\n",
        "2016-08-26 22:58:36,686 INFO  [stdout writer for /usr/bin/python] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1022)) - mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
        "2016-08-26 22:58:36,686 INFO  [stdout writer for /usr/bin/python] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1022)) - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
        "2016-08-26 22:58:36,686 INFO  [stdout writer for /usr/bin/python] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1022)) - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
        "2016-08-26 22:58:36,687 INFO  [stdout writer for /usr/bin/python] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1022)) - mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
        "2016-08-26 22:58:36,687 INFO  [stdout writer for /usr/bin/python] Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1022)) - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
        "2016-08-26 22:58:36,703 INFO  [Executor task launch worker-3] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 39, boot = 5, init = 32, finish = 2\n",
        "2016-08-26 22:58:36,709 INFO  [Executor task launch worker-2] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 41, boot = 3, init = 32, finish = 6\n",
        "2016-08-26 22:58:36,741 INFO  [Executor task launch worker-3] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 1.0 (TID 3). 4411 bytes result sent to driver\n",
        "2016-08-26 22:58:36,742 INFO  [Executor task launch worker-2] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 1.0 (TID 2). 4478 bytes result sent to driver\n",
        "2016-08-26 22:58:36,757 INFO  [Result resolver thread-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 1.0 (TID 3) in 111 ms on localhost (1/2)\n",
        "2016-08-26 22:58:36,762 INFO  [sparkDriver-akka.actor.default-dispatcher-5] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 1 (collect at <ipython-input-9-f03a6e17fb33>:1) finished in 0.124 s\n",
        "2016-08-26 22:58:36,762 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Job finished: collect at <ipython-input-9-f03a6e17fb33>:1, took 0.15055337 s\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = input.flatMap(lambda x: x.split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 22:58:36,770 INFO  [Result resolver thread-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 1.0 (TID 2) in 121 ms on localhost (2/2)\n",
        "2016-08-26 22:58:36,773 INFO  [Result resolver thread-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 1.0, whose tasks have all completed, from pool \n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#words.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordCounts = words.countByValue()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 23:01:19,442 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: countByValue at <ipython-input-13-cef238283d61>:1\n",
        "2016-08-26 23:01:19,443 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 3 (countByValue at <ipython-input-13-cef238283d61>:1) with 2 output partitions (allowLocal=false)\n",
        "2016-08-26 23:01:19,444 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 3(countByValue at <ipython-input-13-cef238283d61>:1)\n",
        "2016-08-26 23:01:19,444 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List()\n",
        "2016-08-26 23:01:19,445 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List()\n",
        "2016-08-26 23:01:19,446 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 3 (PythonRDD[6] at RDD at PythonRDD.scala:43), which has no missing parents\n",
        "2016-08-26 23:01:19,450 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5576) called with curMem=189877, maxMem=280248975\n",
        "2016-08-26 23:01:19,452 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_4 stored as values in memory (estimated size 5.4 KB, free 267.1 MB)\n",
        "2016-08-26 23:01:19,455 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 3 (PythonRDD[6] at RDD at PythonRDD.scala:43)\n",
        "2016-08-26 23:01:19,456 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 3.0 with 2 tasks\n",
        "2016-08-26 23:01:19,459 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 3.0 (TID 6, localhost, PROCESS_LOCAL, 1183 bytes)\n",
        "2016-08-26 23:01:19,460 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 3.0 (TID 7, localhost, PROCESS_LOCAL, 1183 bytes)\n",
        "2016-08-26 23:01:19,461 INFO  [Executor task launch worker-6] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 3.0 (TID 6)\n",
        "2016-08-26 23:01:19,467 INFO  [Executor task launch worker-7] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 3.0 (TID 7)\n",
        "2016-08-26 23:01:19,489 INFO  [stdout writer for /usr/bin/python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/home/cloudera/cnn.txt:0+2570\n",
        "2016-08-26 23:01:19,493 INFO  [stdout writer for /usr/bin/python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/home/cloudera/cnn.txt:2570+2571\n",
        "2016-08-26 23:01:19,498 INFO  [Executor task launch worker-6] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 21, boot = 4, init = 11, finish = 6\n",
        "2016-08-26 23:01:19,512 INFO  [Executor task launch worker-7] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 31, boot = 5, init = 22, finish = 4\n",
        "2016-08-26 23:01:19,528 INFO  [Executor task launch worker-7] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 3.0 (TID 7). 5372 bytes result sent to driver\n",
        "2016-08-26 23:01:19,529 INFO  [Executor task launch worker-6] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 3.0 (TID 6). 5465 bytes result sent to driver\n",
        "2016-08-26 23:01:19,535 INFO  [Result resolver thread-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 3.0 (TID 7) in 66 ms on localhost (1/2)\n",
        "2016-08-26 23:01:19,548 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 3 (countByValue at <ipython-input-13-cef238283d61>:1) finished in 0.087 s\n",
        "2016-08-26 23:01:19,549 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Job finished: countByValue at <ipython-input-13-cef238283d61>:1, took 0.106456263 s\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordCounts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 23:01:19,552 INFO  [Result resolver thread-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 3.0 (TID 6) in 81 ms on localhost (2/2)\n",
        "2016-08-26 23:01:19,555 INFO  [Result resolver thread-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 3.0, whose tasks have all completed, from pool \n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "defaultdict(<type 'int'>, {u'Arbitration': 1, u'limited': 1, u'all': 1, u'petition,': 2, u'rules,': 1, u'rules.': 1, u'Olympics': 1, u'legal': 1, u'decisions': 1, u'to': 25, u'Facebook': 1, u'sorry': 1, u'Now': 2, u'decision.': 2, u'decision)': 1, u'talk,': 1, u'very': 2, u'blanket': 1, u'decision\"': 1, u\"women's\": 1, u'every': 1, u\"I'm\": 1, u'month': 1, u\"we'll\": 2, u'\"unfair': 1, u'--': 2, u'agency.': 1, u'team': 1, u'says': 1, u'punishing': 1, u'me,': 1, u'opting': 1, u'cost': 1, u'implemented': 1, u'even': 1, u'spokesman': 1, u'international': 1, u'appeal': 2, u'told': 3, u'body': 1, u'never': 1, u'let': 1, u\"aren't\": 1, u'change': 1, u'added.': 1, u\"country's\": 1, u'Russians': 2, u\"It's\": 3, u'When': 1, u'brought': 1, u'moral': 1, u'Paralympics,': 1, u'\"The': 3, u'from': 10, u'handled': 1, u'would': 1, u'angry': 1, u'shocked,\"': 1, u'Russia': 7, u'live': 1, u'upheld': 3, u'Russian': 12, u'sort': 1, u'So': 1, u'\"inhuman\"': 1, u'announcing': 1, u'this': 6, u'athlete': 1, u'work': 1, u'believes': 1, u'fulfill': 1, u'CAS': 1, u'mentality.\"': 1, u'allowed': 2, u\"didn't\": 1, u'want': 1, u'sense': 1, u'Games,': 1, u'Several': 1, u'sympathy\"': 1, u'needs': 1, u'Putin': 1, u'independently.': 1, u'responsibilities': 1, u'future.': 1, u'widespread': 1, u'\"My': 1, u'explains': 2, u'Why': 1, u'after': 2, u'membership': 1, u'such': 1, u'appealed': 1, u'a': 5, u'dope?': 1, u'wheelchair': 1, u'While': 1, u'recover.': 1, u'so': 2, u'enter': 1, u'humanity.': 1, u'athletes': 10, u'playing': 1, u\"don't\": 2, u'disabled': 1, u'including': 1, u'differently.': 1, u'ban,': 1, u'ban.': 1, u'its': 4, u'300,000': 1, u'suspension': 2, u'2016': 1, u'petition': 2, u'then': 1, u'them': 2, u'bans': 2, u'they': 10, u'not': 4, u'Putin.': 1, u'Tass': 1, u'01:22': 1, u'IPC': 8, u'stance.': 1, u'labeling': 1, u'Sport': 2, u'principles': 1, u'Sports': 1, u'right.\"': 1, u'our': 3, u'beyond': 1, u'event': 1, u'out': 2, u'389-strong': 1, u'ultimately': 1, u'since': 1, u'00:46': 1, u'\"It': 3, u'families,': 1, u'belief': 1, u'humiliating': 1, u'governing': 1, u'federations': 1, u'support.': 1, u'CNN.': 3, u'People': 1, u'obligations.': 1, u'times': 1, u'place': 1, u'athletes,': 1, u'\"Everyone': 1, u'think': 2, u'first': 3, u'suspend': 1, u'state-run': 1, u'feel': 1, u'one': 1, u'suspended': 1, u'president': 2, u'open': 1, u'millions': 1, u'given': 1, u'cynical': 1, u'U-turn': 1, u'caught': 1, u'system': 3, u'different,\"': 1, u'their': 2, u'lot': 1, u'first.': 1, u'that': 9, u'part': 1, u'believe': 4, u'Craig': 1, u'All-Russian': 2, u'officials': 1, u'were': 1, u'norms,': 1, u'and': 14, u'Court': 1, u'IPC,': 1, u'Alferova': 2, u'\"We\\'ll': 1, u'have': 5, u'afraid': 1, u'do.': 1, u'responsible': 2, u'also': 2, u'online': 1, u'responsibility': 1, u'sure': 1, u'who': 7, u'Games': 5, u'letter': 1, u'nothing': 1, u'The': 6, u'why': 1, u'mistake,\"': 1, u'dream': 1, u'upset': 1, u'barred': 2, u'allegations,': 1, u'not.': 1, u'strong,': 1, u'responded': 1, u\"Alferova's\": 1, u'signatures': 1, u'inspire': 1, u'state-sponsored': 2, u\"month's\": 1, u'(CAS)': 1, u'Janeiro': 1, u'02:37': 1, u'should': 4, u'team,': 1, u'failed': 1, u'going': 2, u'Thursday,': 1, u'hope': 1, u'do': 2, u'means': 1, u'banned': 4, u'de': 1, u'them.\"': 1, u'feels': 2, u'cannot': 2, u'competing': 2, u'anything.': 2, u'calling': 1, u'Disabled': 1, u'Rio': 6, u'she': 3, u'ban': 5, u'them,': 1, u'humiliation': 1, u'Deaf,': 1, u'see': 1, u'individual': 1, u'are': 5, u'sport': 1, u'said': 2, u'clearing': 1, u'boundaries': 1, u'behind': 1, u'future': 1, u\"IPC's\": 2, u'involved.': 1, u'missing': 1, u'Paralympics': 2, u'news': 1, u'received': 1, u'many': 1, u'taking': 1, u'countries,': 1, u'according': 2, u'against': 1, u'participating': 1, u'however,': 1, u'live,': 1, u'simply': 1, u\"couldn't\": 2, u'doping': 7, u'Broken': 1, u\"it's\": 2, u'due': 4, u'been': 5, u'Ludmila': 1, u'reaction': 1, u'(to': 1, u'CAS.': 1, u'Craven': 2, u'understand': 1, u'violations': 1, u'those': 3, u'And': 1, u'Alferova,': 1, u'\"It\\'s': 1, u'will': 3, u'life.': 1, u'aren&#39;t': 1, u'...': 2, u'is': 11, u'it': 3, u'\"This': 1, u'helped': 1, u'itself': 1, u'in': 9, u\"Russia's\": 4, u'Russia,\"': 1, u'said.': 2, u'if': 2, u'Paralympians': 5, u'make': 1, u'\"I': 4, u'member': 2, u'President': 2, u'difficult': 1, u'sport,\"': 1, u'I': 6, u'disagree': 1, u'Committee,\"': 1, u'keep': 3, u'off': 1, u'Bubnova,': 1, u'people.': 2, u'It': 1, u'echoed': 1, u'thought': 1, u'without': 1, u'Have': 1, u'inability': 1, u'the': 40, u'stating': 1, u'If': 1, u'dream.': 1, u'\"Under': 1, u'only.': 1, u'unfair': 1, u'themselves,\"': 1, u'adding': 1, u'had': 2, u'\"enormous': 1, u'Games.\"': 1, u'innocent': 1, u'abilities.\"': 1, u'Philip': 1, u'has': 7, u'Spence': 3, u'(CNN)The': 1, u'tests': 1, u'Many': 1, u'lifted.\"': 1, u'\"medals-over-morals': 1, u'immediately': 1, u'dreams': 1, u'imposed': 1, u'justice.': 1, u'country;': 1, u'page': 1, u'because': 3, u'Bubnova': 1, u'people': 1, u'underwent': 1, u'some': 5, u'International': 1, u'118': 1, u'Paralympics.': 1, u'for': 8, u'Moscow': 1, u'comments': 1, u'everything': 1, u'(IPC)': 1, u'(people)?\"': 1, u'month,': 1, u'violations.': 1, u'denied': 2, u'be': 8, u'step': 1, u'Photos:': 1, u'by': 4, u'on': 9, u'of': 15, u'Paralympic': 13, u'urged': 1, u'IAAF': 2, u'Protest': 1, u'Committee': 2, u'or': 1, u'No': 1, u'into': 1, u'down': 1, u'Kseniya': 1, u'But': 1, u'fighting,': 1, u'training,': 1, u'your': 1, u'her': 1, u'charities,': 1, u'tennis': 1, u'alleged': 2, u'start': 1, u'frustration': 1, u'suspended.': 1, u'suspended,': 1, u'was': 6, u'himself': 1, u'but': 3, u'life.\"': 1, u'\"I\\'m': 1, u'with': 11, u'he': 1, u'made': 1, u\"Craven's\": 1, u'up': 1, u'Society': 2, u'say': 1, u'an': 4, u'as': 2, u'at': 5, u'Committee.': 1, u'politics': 2, u'physical': 1, u'when': 1, u'you': 1, u'01:48': 1, u'reconsider': 1, u'Vladimir': 1, u'friends': 1, u'independently': 1, u'cheated': 1, u'compete': 3, u'decision': 3})"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from operator import itemgetter\n",
      "\n",
      "swd = sorted(wordCounts.items(), key = itemgetter(1), reverse=True)\n",
      "for ele in swd[0:10]:\n",
      "    print ele"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'the', 40)\n",
        "(u'to', 25)\n",
        "(u'of', 15)\n",
        "(u'and', 14)\n",
        "(u'Paralympic', 13)\n",
        "(u'Russian', 12)\n",
        "(u'is', 11)\n",
        "(u'with', 11)\n",
        "(u'from', 10)\n",
        "(u'athletes', 10)\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def square(a):\n",
      "    return a ** 2\n",
      "square(4)\n",
      "\n",
      "square2 = lambda e : e **2\n",
      "square2(6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "36"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Spark SQL"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.sql import SQLContext\n",
      "sqlContext = SQLContext(sc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 23:47:19,962 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 17\n",
        "2016-08-26 23:47:19,964 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_17\n",
        "2016-08-26 23:47:19,964 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_17 of size 14224 dropped from memory (free 279883648)\n",
        "2016-08-26 23:47:19,966 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 17\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_file = \"file:///home/cloudera/ratings.tsv\"\n",
      "raw_data = sc.textFile(data_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 23:52:42,264 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(87505) called with curMem=365327, maxMem=280248975\n",
        "2016-08-26 23:52:42,270 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_18 stored as values in memory (estimated size 85.5 KB, free 266.8 MB)\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "header = raw_data.first()\n",
      "print header\n",
      "#skip_data = raw_data.filter(lambda line: line != header)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\t0\t4\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 23:53:01,799 INFO  [Thread-2] mapred.FileInputFormat (FileInputFormat.java:listStatus(247)) - Total input paths to process : 1\n",
        "2016-08-26 23:53:01,812 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: runJob at PythonRDD.scala:296\n",
        "2016-08-26 23:53:01,813 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 14 (runJob at PythonRDD.scala:296) with 1 output partitions (allowLocal=true)\n",
        "2016-08-26 23:53:01,813 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 15(runJob at PythonRDD.scala:296)\n",
        "2016-08-26 23:53:01,813 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List()\n",
        "2016-08-26 23:53:01,816 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List()\n",
        "2016-08-26 23:53:01,816 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 15 (PythonRDD[57] at RDD at PythonRDD.scala:43), which has no missing parents\n",
        "2016-08-26 23:53:01,817 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(4424) called with curMem=452832, maxMem=280248975\n",
        "2016-08-26 23:53:01,823 INFO  [sparkDriver-akka.actor.default-dispatcher-15] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_19 stored as values in memory (estimated size 4.3 KB, free 266.8 MB)\n",
        "2016-08-26 23:53:01,824 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 1 missing tasks from Stage 15 (PythonRDD[57] at RDD at PythonRDD.scala:43)\n",
        "2016-08-26 23:53:01,824 INFO  [sparkDriver-akka.actor.default-dispatcher-15] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 15.0 with 1 tasks\n",
        "2016-08-26 23:53:01,827 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 15.0 (TID 219, localhost, PROCESS_LOCAL, 1187 bytes)\n",
        "2016-08-26 23:53:01,831 INFO  [Executor task launch worker-14] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 15.0 (TID 219)\n",
        "2016-08-26 23:53:01,846 INFO  [stdout writer for /usr/bin/python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/home/cloudera/ratings.tsv:0+1631806\n",
        "2016-08-26 23:53:01,857 INFO  [Executor task launch worker-14] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 22, boot = 6, init = 16, finish = 0\n",
        "2016-08-26 23:53:01,866 INFO  [Result resolver thread-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 15.0 (TID 219) in 37 ms on localhost (1/1)\n",
        "2016-08-26 23:53:01,866 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 15 (runJob at PythonRDD.scala:296) finished in 0.039 s\n",
        "2016-08-26 23:53:01,867 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Job finished: runJob at PythonRDD.scala:296, took 0.053977793 s\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.sql import Row\n",
      "\n",
      "tab_data = raw_data.map(lambda l: l.split(\"\\t\"))\n",
      "row_data = tab_data.map(lambda p: Row(\n",
      "    userid=p[0], \n",
      "    itemid=p[1],\n",
      "    rating=int(p[2])\n",
      "    )\n",
      ")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rt = sqlContext.inferSchema(row_data)\n",
      "rt.registerTempTable(\"ratings\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 23:56:43,286 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: runJob at PythonRDD.scala:296\n",
        "2016-08-26 23:56:43,286 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 15 (runJob at PythonRDD.scala:296) with 1 output partitions (allowLocal=true)\n",
        "2016-08-26 23:56:43,286 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 16(runJob at PythonRDD.scala:296)\n",
        "2016-08-26 23:56:43,286 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List()\n",
        "2016-08-26 23:56:43,288 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List()\n",
        "2016-08-26 23:56:43,288 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 16 (PythonRDD[59] at RDD at PythonRDD.scala:43), which has no missing parents\n",
        "2016-08-26 23:56:43,289 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5256) called with curMem=457256, maxMem=280248975\n",
        "2016-08-26 23:56:43,290 INFO  [sparkDriver-akka.actor.default-dispatcher-16] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_20 stored as values in memory (estimated size 5.1 KB, free 266.8 MB)\n",
        "2016-08-26 23:56:43,290 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 1 missing tasks from Stage 16 (PythonRDD[59] at RDD at PythonRDD.scala:43)\n",
        "2016-08-26 23:56:43,290 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 16.0 with 1 tasks\n",
        "2016-08-26 23:56:43,293 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 16.0 (TID 220, localhost, PROCESS_LOCAL, 1187 bytes)\n",
        "2016-08-26 23:56:43,294 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 16.0 (TID 220)\n",
        "2016-08-26 23:56:43,306 INFO  [stdout writer for /usr/bin/python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/home/cloudera/ratings.tsv:0+1631806\n",
        "2016-08-26 23:56:43,316 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 21, boot = 6, init = 15, finish = 0\n",
        "2016-08-26 23:56:43,319 INFO  [Result resolver thread-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 16.0 (TID 220) in 27 ms on localhost (1/1)\n",
        "2016-08-26 23:56:43,320 INFO  [Result resolver thread-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
        "2016-08-26 23:56:43,320 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 16 (runJob at PythonRDD.scala:296) finished in 0.026 s\n",
        "2016-08-26 23:56:43,321 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Job finished: runJob at PythonRDD.scala:296, took 0.034647334 s\n",
        "2016-08-26 23:56:43,324 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 16.0 (TID 220). 1808 bytes result sent to driver\n",
        "2016-08-26 23:56:43,336 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: runJob at PythonRDD.scala:296\n",
        "2016-08-26 23:56:43,336 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 16 (runJob at PythonRDD.scala:296) with 1 output partitions (allowLocal=true)\n",
        "2016-08-26 23:56:43,336 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 17(runJob at PythonRDD.scala:296)\n",
        "2016-08-26 23:56:43,336 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List()\n",
        "2016-08-26 23:56:43,337 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List()\n",
        "2016-08-26 23:56:43,338 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 17 (PythonRDD[61] at RDD at PythonRDD.scala:43), which has no missing parents\n",
        "2016-08-26 23:56:43,339 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(5792) called with curMem=462512, maxMem=280248975\n",
        "2016-08-26 23:56:43,339 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_21 stored as values in memory (estimated size 5.7 KB, free 266.8 MB)\n",
        "2016-08-26 23:56:43,340 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 1 missing tasks from Stage 17 (PythonRDD[61] at RDD at PythonRDD.scala:43)\n",
        "2016-08-26 23:56:43,340 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 17.0 with 1 tasks\n",
        "2016-08-26 23:56:43,344 INFO  [sparkDriver-akka.actor.default-dispatcher-16] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 17.0 (TID 221, localhost, PROCESS_LOCAL, 1187 bytes)\n",
        "2016-08-26 23:56:43,345 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 17.0 (TID 221)\n",
        "2016-08-26 23:56:43,352 INFO  [stdout writer for /usr/bin/python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/home/cloudera/ratings.tsv:0+1631806\n",
        "2016-08-26 23:56:43,358 INFO  [Executor task launch worker-15] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 12, boot = 2, init = 9, finish = 1\n",
        "2016-08-26 23:56:43,361 INFO  [Result resolver thread-3] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 17.0 (TID 221) in 16 ms on localhost (1/1)\n",
        "2016-08-26 23:56:43,361 INFO  [Result resolver thread-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
        "2016-08-26 23:56:43,362 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 17 (runJob at PythonRDD.scala:296) finished in 0.015 s\n",
        "2016-08-26 23:56:43,362 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Job finished: runJob at PythonRDD.scala:296, took 0.026269397 s\n",
        "2016-08-26 23:56:43,370 INFO  [Executor task launch worker-15] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 17.0 (TID 221). 1927 bytes result sent to driver\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rt.printSchema()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "root\n",
        " |-- itemid: string (nullable = true)\n",
        " |-- rating: integer (nullable = true)\n",
        " |-- userid: string (nullable = true)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ratings_data = sqlContext.sql(\"\"\"\n",
      "    SELECT userid,avg(rating) from ratings group by userid\n",
      "\"\"\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 23:57:27,838 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 19\n",
        "2016-08-26 23:57:27,839 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_19\n",
        "2016-08-26 23:57:27,839 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_19 of size 4424 dropped from memory (free 279785095)\n",
        "2016-08-26 23:57:27,839 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 19\n",
        "2016-08-26 23:57:27,840 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 20\n",
        "2016-08-26 23:57:27,840 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_20\n",
        "2016-08-26 23:57:27,840 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_20 of size 5256 dropped from memory (free 279790351)\n",
        "2016-08-26 23:57:27,840 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 20\n",
        "2016-08-26 23:57:27,840 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing broadcast 21\n",
        "2016-08-26 23:57:27,841 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManager (Logging.scala:logInfo(59)) - Removing block broadcast_21\n",
        "2016-08-26 23:57:27,841 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_21 of size 5792 dropped from memory (free 279796143)\n",
        "2016-08-26 23:57:27,841 INFO  [Spark Context Cleaner] spark.ContextCleaner (Logging.scala:logInfo(59)) - Cleaned broadcast 21\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ratings_data.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 23:57:46,827 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: runJob at PythonRDD.scala:296\n",
        "2016-08-26 23:57:46,829 INFO  [sparkDriver-akka.actor.default-dispatcher-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Registering RDD 69 (mapPartitions at Exchange.scala:48)\n",
        "2016-08-26 23:57:46,829 INFO  [sparkDriver-akka.actor.default-dispatcher-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 17 (runJob at PythonRDD.scala:296) with 1 output partitions (allowLocal=true)\n",
        "2016-08-26 23:57:46,830 INFO  [sparkDriver-akka.actor.default-dispatcher-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 18(runJob at PythonRDD.scala:296)\n",
        "2016-08-26 23:57:46,830 INFO  [sparkDriver-akka.actor.default-dispatcher-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List(Stage 19)\n",
        "2016-08-26 23:57:46,845 INFO  [sparkDriver-akka.actor.default-dispatcher-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List(Stage 19)\n",
        "2016-08-26 23:57:46,847 INFO  [sparkDriver-akka.actor.default-dispatcher-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 19 (MapPartitionsRDD[69] at mapPartitions at Exchange.scala:48), which has no missing parents\n",
        "2016-08-26 23:57:46,851 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(12288) called with curMem=452832, maxMem=280248975\n",
        "2016-08-26 23:57:46,852 INFO  [sparkDriver-akka.actor.default-dispatcher-2] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_22 stored as values in memory (estimated size 12.0 KB, free 266.8 MB)\n",
        "2016-08-26 23:57:46,852 INFO  [sparkDriver-akka.actor.default-dispatcher-2] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 19 (MapPartitionsRDD[69] at mapPartitions at Exchange.scala:48)\n",
        "2016-08-26 23:57:46,854 INFO  [sparkDriver-akka.actor.default-dispatcher-2] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 19.0 with 2 tasks\n",
        "2016-08-26 23:57:46,855 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 19.0 (TID 222, localhost, PROCESS_LOCAL, 1176 bytes)\n",
        "2016-08-26 23:57:46,855 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 19.0 (TID 223, localhost, PROCESS_LOCAL, 1176 bytes)\n",
        "2016-08-26 23:57:46,856 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 19.0 (TID 222)\n",
        "2016-08-26 23:57:46,861 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 19.0 (TID 223)\n",
        "2016-08-26 23:57:46,884 INFO  [stdout writer for /usr/bin/python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/home/cloudera/ratings.tsv:0+1631806\n",
        "2016-08-26 23:57:46,887 INFO  [stdout writer for /usr/bin/python] rdd.HadoopRDD (Logging.scala:logInfo(59)) - Input split: file:/home/cloudera/ratings.tsv:1631806+1631806\n",
        "2016-08-26 23:57:49,815 INFO  [Executor task launch worker-17] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 2939, boot = 5, init = 111, finish = 2823\n",
        "2016-08-26 23:57:49,940 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 3067, boot = 7, init = 60, finish = 3000\n",
        "2016-08-26 23:57:50,163 INFO  [Executor task launch worker-17] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 19.0 (TID 223). 2067 bytes result sent to driver\n",
        "2016-08-26 23:57:50,196 INFO  [Result resolver thread-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 19.0 (TID 223) in 3340 ms on localhost (1/2)\n",
        "2016-08-26 23:57:50,231 INFO  [Result resolver thread-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 19.0 (TID 222) in 3375 ms on localhost (2/2)\n",
        "2016-08-26 23:57:50,232 INFO  [Result resolver thread-0] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
        "2016-08-26 23:57:50,231 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 19 (mapPartitions at Exchange.scala:48) finished in 3.375 s\n",
        "2016-08-26 23:57:50,232 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - looking for newly runnable stages\n",
        "2016-08-26 23:57:50,232 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - running: Set()\n",
        "2016-08-26 23:57:50,232 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - waiting: Set(Stage 18)\n",
        "2016-08-26 23:57:50,232 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - failed: Set()\n",
        "2016-08-26 23:57:50,236 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 19.0 (TID 222). 2067 bytes result sent to driver\n",
        "2016-08-26 23:57:50,240 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents for Stage 18: List()\n",
        "2016-08-26 23:57:50,241 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 18 (PythonRDD[73] at RDD at PythonRDD.scala:43), which is now runnable\n",
        "2016-08-26 23:57:50,247 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(15488) called with curMem=465120, maxMem=280248975\n",
        "2016-08-26 23:57:50,248 INFO  [sparkDriver-akka.actor.default-dispatcher-13] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_23 stored as values in memory (estimated size 15.1 KB, free 266.8 MB)\n",
        "2016-08-26 23:57:50,249 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 1 missing tasks from Stage 18 (PythonRDD[73] at RDD at PythonRDD.scala:43)\n",
        "2016-08-26 23:57:50,252 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 18.0 with 1 tasks\n",
        "2016-08-26 23:57:50,253 INFO  [sparkDriver-akka.actor.default-dispatcher-2] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 18.0 (TID 224, localhost, PROCESS_LOCAL, 948 bytes)\n",
        "2016-08-26 23:57:50,253 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 18.0 (TID 224)\n",
        "2016-08-26 23:57:50,263 INFO  [stdout writer for /usr/bin/python] storage.BlockFetcherIterator$BasicBlockFetcherIterator (Logging.scala:logInfo(59)) - maxBytesInFlight: 50331648, targetRequestSize: 10066329\n",
        "2016-08-26 23:57:50,263 INFO  [stdout writer for /usr/bin/python] storage.BlockFetcherIterator$BasicBlockFetcherIterator (Logging.scala:logInfo(59)) - Getting 2 non-empty blocks out of 2 blocks\n",
        "2016-08-26 23:57:50,263 INFO  [stdout writer for /usr/bin/python] storage.BlockFetcherIterator$BasicBlockFetcherIterator (Logging.scala:logInfo(59)) - Started 0 remote fetches in 0 ms\n",
        "2016-08-26 23:57:50,275 INFO  [Executor task launch worker-16] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 19, boot = 3, init = 16, finish = 0\n",
        "2016-08-26 23:57:50,278 INFO  [Result resolver thread-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 18.0 (TID 224) in 24 ms on localhost (1/1)\n",
        "2016-08-26 23:57:50,278 INFO  [Result resolver thread-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
        "2016-08-26 23:57:50,279 INFO  [sparkDriver-akka.actor.default-dispatcher-13] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 18 (runJob at PythonRDD.scala:296) finished in 0.022 s\n",
        "2016-08-26 23:57:50,279 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Job finished: runJob at PythonRDD.scala:296, took 3.451074921 s\n",
        "2016-08-26 23:57:50,284 INFO  [Executor task launch worker-16] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 18.0 (TID 224). 1410 bytes result sent to driver\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "[Row(userid=u'6698', c1=4.333333333333333),\n",
        " Row(userid=u'5518', c1=4.25),\n",
        " Row(userid=u'8029', c1=5.0),\n",
        " Row(userid=u'5400', c1=4.0555555555555554),\n",
        " Row(userid=u'1297', c1=3.0617283950617282),\n",
        " Row(userid=u'6247', c1=3.6666666666666665),\n",
        " Row(userid=u'2232', c1=3.606060606060606),\n",
        " Row(userid=u'2070', c1=3.4680851063829787),\n",
        " Row(userid=u'4465', c1=4.6086956521739131),\n",
        " Row(userid=u'395', c1=4.0673076923076925)]"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}