{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = 3 \n",
      "b = 5 \n",
      "a + b"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "8"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import findspark\n",
      "findspark.init('/usr/lib/spark/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pyspark\n",
      "sc = pyspark.SparkContext()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 22:38:20,583 WARN  [Thread-2] util.Utils (Logging.scala:logWarning(71)) - Your hostname, quickstart.cloudera resolves to a loopback address: 127.0.0.1; using 192.168.188.128 instead (on interface eth1)\n",
        "2016-08-26 22:38:20,597 WARN  [Thread-2] util.Utils (Logging.scala:logWarning(71)) - Set SPARK_LOCAL_IP if you need to bind to another address\n",
        "2016-08-26 22:38:20,936 INFO  [Thread-2] spark.SecurityManager (Logging.scala:logInfo(59)) - Changing view acls to: cloudera\n",
        "2016-08-26 22:38:20,937 INFO  [Thread-2] spark.SecurityManager (Logging.scala:logInfo(59)) - Changing modify acls to: cloudera\n",
        "2016-08-26 22:38:20,937 INFO  [Thread-2] spark.SecurityManager (Logging.scala:logInfo(59)) - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(cloudera); users with modify permissions: Set(cloudera)\n",
        "2016-08-26 22:38:21,441 INFO  [sparkDriver-akka.actor.default-dispatcher-3] slf4j.Slf4jLogger (Slf4jLogger.scala:applyOrElse(80)) - Slf4jLogger started\n",
        "2016-08-26 22:38:21,541 INFO  [sparkDriver-akka.actor.default-dispatcher-5] Remoting (Slf4jLogger.scala:apply$mcV$sp(74)) - Starting remoting\n",
        "2016-08-26 22:38:21,735 INFO  [sparkDriver-akka.actor.default-dispatcher-4] Remoting (Slf4jLogger.scala:apply$mcV$sp(74)) - Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.188.128:58669]\n",
        "2016-08-26 22:38:21,736 INFO  [sparkDriver-akka.actor.default-dispatcher-4] Remoting (Slf4jLogger.scala:apply$mcV$sp(74)) - Remoting now listens on addresses: [akka.tcp://sparkDriver@192.168.188.128:58669]\n",
        "2016-08-26 22:38:21,751 INFO  [Thread-2] util.Utils (Logging.scala:logInfo(59)) - Successfully started service 'sparkDriver' on port 58669.\n",
        "2016-08-26 22:38:21,771 INFO  [Thread-2] spark.SparkEnv (Logging.scala:logInfo(59)) - Registering MapOutputTracker\n",
        "2016-08-26 22:38:21,781 INFO  [Thread-2] spark.SparkEnv (Logging.scala:logInfo(59)) - Registering BlockManagerMaster\n",
        "2016-08-26 22:38:21,808 INFO  [Thread-2] storage.DiskBlockManager (Logging.scala:logInfo(59)) - Created local directory at /tmp/spark-local-20160826223821-6c2e\n",
        "2016-08-26 22:38:21,867 INFO  [Thread-2] util.Utils (Logging.scala:logInfo(59)) - Successfully started service 'Connection manager for block manager' on port 56635.\n",
        "2016-08-26 22:38:21,869 INFO  [Thread-2] network.ConnectionManager (Logging.scala:logInfo(59)) - Bound socket to port 56635 with id = ConnectionManagerId(192.168.188.128,56635)\n",
        "2016-08-26 22:38:21,874 INFO  [Thread-2] storage.MemoryStore (Logging.scala:logInfo(59)) - MemoryStore started with capacity 267.3 MB\n",
        "2016-08-26 22:38:21,891 INFO  [Thread-2] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Trying to register BlockManager\n",
        "2016-08-26 22:38:21,893 INFO  [sparkDriver-akka.actor.default-dispatcher-5] storage.BlockManagerMasterActor (Logging.scala:logInfo(59)) - Registering block manager 192.168.188.128:56635 with 267.3 MB RAM\n",
        "2016-08-26 22:38:21,898 INFO  [Thread-2] storage.BlockManagerMaster (Logging.scala:logInfo(59)) - Registered BlockManager\n",
        "2016-08-26 22:38:21,920 INFO  [Thread-2] spark.HttpFileServer (Logging.scala:logInfo(59)) - HTTP File server directory is /tmp/spark-c5dc06a4-8401-47a1-b0d1-040fcabce50a\n",
        "2016-08-26 22:38:21,924 INFO  [Thread-2] spark.HttpServer (Logging.scala:logInfo(59)) - Starting HTTP Server\n",
        "2016-08-26 22:38:22,071 INFO  [Thread-2] server.Server (Server.java:doStart(272)) - jetty-8.y.z-SNAPSHOT\n",
        "2016-08-26 22:38:22,107 INFO  [Thread-2] server.AbstractConnector (AbstractConnector.java:doStart(338)) - Started SocketConnector@0.0.0.0:54512\n",
        "2016-08-26 22:38:22,107 INFO  [Thread-2] util.Utils (Logging.scala:logInfo(59)) - Successfully started service 'HTTP file server' on port 54512.\n",
        "2016-08-26 22:38:22,473 INFO  [Thread-2] server.Server (Server.java:doStart(272)) - jetty-8.y.z-SNAPSHOT\n",
        "2016-08-26 22:38:22,483 INFO  [Thread-2] server.AbstractConnector (AbstractConnector.java:doStart(338)) - Started SelectChannelConnector@0.0.0.0:4040\n",
        "2016-08-26 22:38:22,483 INFO  [Thread-2] util.Utils (Logging.scala:logInfo(59)) - Successfully started service 'SparkUI' on port 4040.\n",
        "2016-08-26 22:38:22,486 INFO  [Thread-2] ui.SparkUI (Logging.scala:logInfo(59)) - Started SparkUI at http://192.168.188.128:4040\n",
        "2016-08-26 22:38:23,147 WARN  [Thread-2] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
        "2016-08-26 22:38:23,721 INFO  [sparkDriver-akka.actor.default-dispatcher-5] util.AkkaUtils (Logging.scala:logInfo(59)) - Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@192.168.188.128:58669/user/HeartbeatReceiver\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## First Spark Program"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd=sc.parallelize([1,2,3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.reduce(lambda a,b: a* b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2016-08-26 22:40:06,609 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Starting job: reduce at <ipython-input-5-b44a8bdc7f8c>:1\n",
        "2016-08-26 22:40:06,636 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Got job 0 (reduce at <ipython-input-5-b44a8bdc7f8c>:1) with 2 output partitions (allowLocal=false)\n",
        "2016-08-26 22:40:06,638 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Final stage: Stage 0(reduce at <ipython-input-5-b44a8bdc7f8c>:1)\n",
        "2016-08-26 22:40:06,638 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Parents of final stage: List()\n",
        "2016-08-26 22:40:06,651 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Missing parents: List()\n",
        "2016-08-26 22:40:06,663 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting Stage 0 (PythonRDD[1] at RDD at PythonRDD.scala:43), which has no missing parents\n",
        "2016-08-26 22:40:06,728 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - ensureFreeSpace(3528) called with curMem=0, maxMem=280248975\n",
        "2016-08-26 22:40:06,730 INFO  [sparkDriver-akka.actor.default-dispatcher-3] storage.MemoryStore (Logging.scala:logInfo(59)) - Block broadcast_0 stored as values in memory (estimated size 3.4 KB, free 267.3 MB)\n",
        "2016-08-26 22:40:06,899 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Submitting 2 missing tasks from Stage 0 (PythonRDD[1] at RDD at PythonRDD.scala:43)\n",
        "2016-08-26 22:40:06,900 INFO  [sparkDriver-akka.actor.default-dispatcher-3] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Adding task set 0.0 with 2 tasks\n",
        "2016-08-26 22:40:06,928 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1060 bytes)\n",
        "2016-08-26 22:40:06,942 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1067 bytes)\n",
        "2016-08-26 22:40:06,945 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(59)) - Running task 0.0 in stage 0.0 (TID 0)\n",
        "2016-08-26 22:40:06,947 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(59)) - Running task 1.0 in stage 0.0 (TID 1)\n",
        "2016-08-26 22:40:07,260 INFO  [Executor task launch worker-1] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 259, boot = 242, init = 16, finish = 1\n",
        "2016-08-26 22:40:07,276 INFO  [Executor task launch worker-0] python.PythonRDD (Logging.scala:logInfo(59)) - Times: total = 274, boot = 262, init = 12, finish = 0\n",
        "2016-08-26 22:40:07,296 INFO  [Executor task launch worker-0] executor.Executor (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 0.0 (TID 0). 621 bytes result sent to driver\n",
        "2016-08-26 22:40:07,297 INFO  [Executor task launch worker-1] executor.Executor (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 0.0 (TID 1). 621 bytes result sent to driver\n",
        "2016-08-26 22:40:07,318 INFO  [Result resolver thread-0] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 0.0 in stage 0.0 (TID 0) in 390 ms on localhost (1/2)\n",
        "2016-08-26 22:40:07,327 INFO  [Result resolver thread-1] scheduler.TaskSetManager (Logging.scala:logInfo(59)) - Finished task 1.0 in stage 0.0 (TID 1) in 387 ms on localhost (2/2)\n",
        "2016-08-26 22:40:07,328 INFO  [sparkDriver-akka.actor.default-dispatcher-4] scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 0 (reduce at <ipython-input-5-b44a8bdc7f8c>:1) finished in 0.418 s\n",
        "2016-08-26 22:40:07,334 INFO  [Result resolver thread-1] scheduler.TaskSchedulerImpl (Logging.scala:logInfo(59)) - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
        "2016-08-26 22:40:07,341 INFO  [Thread-2] spark.SparkContext (Logging.scala:logInfo(59)) - Job finished: reduce at <ipython-input-5-b44a8bdc7f8c>:1, took 0.730569943 s\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "6"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}